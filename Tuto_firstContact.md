![Scilex](img/projectLogoScilex.png)

# ğŸš€ SciLEx Tutorial

## 0. ğŸ”‘ Obtain the API Keys You Need

\:heavy\_plus\_sign: [Create a Zotero API key](https://www.zotero.org/support/dev/web_api/v3/start)
\:heavy\_plus\_sign: Create accounts for the following APIs:

* [Semantic Scholar](https://www.semanticscholar.org/product/api/tutorial) (optional â€“ allows a higher rate limit â¬†ï¸, but takes time â³)
* [Springer](https://dev.springernature.com/) (mandatory if selected as a source ğŸ“š)
* [IEEE](https://developer.ieee.org/) (mandatory if selected as a source âš¡)
* [Elsevier](https://dev.elsevier.com/) (mandatory if selected as a source ğŸ§ª)

---

## 1. ğŸ›  Clone the Repository and Install Requirements

**Option 1: Using uv (recommended - fast):**
```bash
git clone https://github.com/datalogism/SciLEx.git
cd SciLEx
uv sync
```

**Option 2: Using pip (traditional):**
```bash
git clone https://github.com/datalogism/SciLEx.git
cd SciLEx
pip install -r requirements.txt
```

---

## 2. ğŸ“ Create and Configure Your Files

1. **Copy the API configuration template:**

   ```bash
   cp src/api.config.yml.example src/api.config.yml
   ```

2. **Edit `src/api.config.yml` with your API credentials:**

   * **Zotero API Key**: [Create here](https://www.zotero.org/settings/keys)
   * **IEEE API Key**: [Register at IEEE Developer Portal](https://developer.ieee.org/)
   * **Elsevier API Key**: [Register at Elsevier Developer Portal](https://dev.elsevier.com/)
   * **Springer API Key**: [Register at Springer Nature Developer Portal](https://dev.springernature.com/)
   * **Semantic Scholar API Key**: Optional, [register here](https://www.semanticscholar.org/product/api/tutorial)

3. **Update your main configuration in [`src/scilex.config.yml`](src/scilex.config.yml):**

```yaml
aggregate_txt_filter: true         # Filter articles based on the given keywords ğŸ”
aggregate_get_citations: true      # Collect all citations during aggregation ğŸ“‘
aggregate_file: 'aggregated_data.csv'  # Aggregated results will be saved here ğŸ’¾
apis:
  - DBLP
  - Arxiv
  - OpenAlex
  - SemanticScholar
collect: true                       # Flag to enable or disable collection âœ…
collect_name: graphrag              # Name for this collection ğŸ·
email: YOUR_MAIL                    # Email used for API requests ğŸ“§
fields:                             # Fields to search for keywords ğŸ§©
  - title
  - abstract
keywords:                            # Two keyword groups for collection ğŸ’¡
  - ["RAG", "LLM", "agent"]         # Group 1: Any of these keywords
  - ["Knowledge Graph"]             # Group 2: Must also match this (dual mode)
                                     # OR: Set second group to [] for single mode
max_articles_per_query: 1000        # Articles per query (-1 = unlimited, 1000 recommended)
output_dir: output
years:
  - 2023
  - 2024
  - 2025

# Semantic Scholar API mode (only if using SemanticScholar)
semantic_scholar_mode: bulk      # "regular" (default, works with standard API keys)
                                    # "bulk" (requires higher-tier access, 10x faster)

# Optional: Quality filters (see scilex.config.yml.example for details)
quality_filters:
  # ItemType Filtering (Whitelist) - NEW!
  enable_itemtype_filter: false          # Enable to only keep specific publication types
  allowed_item_types:                    # Only these types will be kept (others removed)
    - journalArticle                     # Example: Focus on peer-reviewed work only
    - conferencePaper

  enable_itemtype_bypass: true           # Fast-track trusted publications (~50% speedup)
  bypass_item_types: [journalArticle, conferencePaper]
  apply_citation_filter: true            # Time-aware citation filtering
  apply_relevance_ranking: true          # Composite scoring
  max_papers: 1000                       # Keep top N most relevant (null = keep all)
```

---

## 3. â–¶ï¸ Run Your Collection

Once the previous steps are complete, run the collection from the library source:

```bash
python src/run_collecte.py
```

You'll see real-time progress bars for each API. Expected time: ~5-15 minutes per API.

**Optional:** Control logging verbosity:
```bash
LOG_LEVEL=INFO python src/run_collecte.py   # Detailed logs
LOG_LEVEL=DEBUG python src/run_collecte.py  # Full debugging
```

---

## 4. ğŸ“¦ Aggregate the Collection

After collecting all papers, create the final aggregated file:

```bash
python src/aggregate_collect.py
```

**Common flags:**
```bash
--skip-citations        # Skip citation fetching (faster)
--workers N             # Parallel workers for citations (default: 3)
--profile               # Show performance statistics
--resume                # Resume from checkpoint if interrupted
--no-cache              # Disable citation cache (slower, not recommended)
```

---

## 5. ğŸ“‚ View the Results

Results are saved in a dedicated directory inside `./output_dir`. Each directory is named according to the `collect_name` in your configuration:

```
output/collect_name_YYYYMMDD_HHMMSS/
â”œâ”€â”€ SemanticScholar/      # Individual API results ğŸ“š
â”œâ”€â”€ OpenAlex/
â”œâ”€â”€ DBLP/
â”‚   â”œâ”€â”€ 0/                # Query ID
â”‚   â”‚   â”œâ”€â”€ page_1        # Result pages (JSON)
â”‚   â”‚   â””â”€â”€ page_2
â”œâ”€â”€ config_used.yml       # Local copy of the configuration ğŸ“
â”œâ”€â”€ aggregated_data.csv   # Aggregated results ğŸ’¾
â””â”€â”€ citation_cache.db     # Citation cache for faster re-runs
```

**CSV Output:** Contains `title`, `authors`, `abstract`, `DOI`, `URL`, `year`, `itemType`, and enrichment fields like `citation_count`, `quality_score`, `relevance_score`.

---

## 6. ğŸ”„ Push Results to Zotero

If you have your Zotero API key, you can push the `aggregated_data.csv` content to a Zotero library. Note that free Zotero accounts have storage limits, so manual filtering is recommended.

```bash
python src/push_to_zotero.py
```

This will create a new collection based on the `collect_name` defined in your configuration ğŸ.

**Performance:** 500 papers upload in ~10-15 seconds (optimized with bulk API).

---

## ğŸ› Troubleshooting

**Empty results?** Check keywords aren't too specific, try single group mode (`keywords: [["term1", "term2"], []]`)

**Rate limit errors (429)?** Reduce rate limits in `api.config.yml` (e.g., `DBLP: 1.0`)

